Starting a run with:
{'data_params': {'augmentation': False,
                 'classes': 4,
                 'data_path': 'D:/LaureaMagistrale/Tesi Magistrale/MICCAI '
                              'Data/',
                 'dataset': 'brats2018',
                 'load': False,
                 'normalization': 'full_volume_mean',
                 'num_workers': 0,
                 'split': 0.8,
                 'threshold': 1e-11,
                 'train_batch_size': 4,
                 'vol_crop_dim': [64, 64, 64],
                 'vol_train_samples': 32,
                 'vol_val_samples': 4},
 'exp_params': {'inChannels': 4,
                'inModalities': 4,
                'lr': 3.0019475471293287e-05},
 'model_params': {'base_n_filter': 8, 'in_channels': 4, 'n_classes': 4},
 'sweep_params': {'ntrials': 1,
                  'project': 'MICCAI 2018 Medical Image Segmentation'},
 'trainer_params': {'accelerator': 'gpu',
                    'devices': 1,
                    'limit_val_batches': 0,
                    'max_steps': 100,
                    'max_time': '00:00:10:00',
                    'num_sanity_val_steps': 0},
 'wandb_params': {'manual_seed': 42,
                  'save_dir': 'D:/LaureaMagistrale/MedicalZooPytorch/results/logs'}}
Mode: train Subvolume samples to generate:  32  Volumes:  195
Mode: val Subvolume samples to generate:  4  Volumes:  90
So far so good!
D:\IDEs\anaconda3\envs\AML\lib\site-packages\pytorch_lightning\loggers\wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(