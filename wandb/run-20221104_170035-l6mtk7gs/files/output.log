Starting a run with:
{'data_params': {'augmentation': False,
                 'classes': 4,
                 'data_path': 'D:/LaureaMagistrale/Tesi Magistrale/MICCAI '
                              'Data/',
                 'dataset': 'brats2018',
                 'load': False,
                 'normalization': 'full_volume_mean',
                 'num_workers': 0,
                 'split': 0.8,
                 'threshold': 1e-11,
                 'train_batch_size': 4,
                 'vol_crop_dim': [64, 64, 64],
                 'vol_train_samples': 32,
                 'vol_val_samples': 4},
 'exp_params': {'alpha': 0.99,
                'inChannels': 4,
                'inModalities': 4,
                'lr': 0.02789559510222748,
                'momentum': 0.7262466397468035,
                'num_iters': 100,
                'weight_decay': 0.027499347821316952},
 'model_params': {'base_n_filter': 8, 'in_channels': 4, 'n_classes': 4},
 'sweep_params': {'ntrials': 1,
                  'project': 'MICCAI 2018 Medical Image Segmentation'},
 'trainer_params': {'accelerator': 'gpu',
                    'devices': 1,
                    'limit_val_batches': 0,
                    'log_every_n_steps': 50,
                    'max_steps': 100,
                    'max_time': '00:00:10:00',
                    'num_sanity_val_steps': 0,
                    'val_check_interval': 0.5},
 'training_params': {'ckpt_path': None,
                     'debug': True,
                     'n_models_to_save': 10,
                     'resume_train': False},
 'wandb_params': {'ckpt_name': 'UNet3D',
                  'manual_seed': 42,
                  'model_name': 'UNet3D',
                  'save_dir': 'D:/LaureaMagistrale/MedicalZooPytorch/results/logs'}}
Mode: train Subvolume samples to generate:  32  Volumes:  195
Mode: val Subvolume samples to generate:  4  Volumes:  90
D:\IDEs\anaconda3\envs\AML\lib\site-packages\pytorch_lightning\loggers\wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
So far so good!
====== Starting Training UNet3D from scratch ======
Mode: train Subvolume samples to generate:  32  Volumes:  195
Mode: val Subvolume samples to generate:  4  Volumes:  90
Sanity Checking: 0it [00:00, ?it/s]
D:\IDEs\anaconda3\envs\AML\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
